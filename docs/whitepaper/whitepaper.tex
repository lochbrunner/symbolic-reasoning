\documentclass{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{import}

\subimport{docs/libs/}{init}

\usetikzlibrary{positioning}


\usepackage[
backend=biber,
% style=alphabetic,
sorting=none
]{biblatex}

\addbibresource{whitepaper.bib}

\title{Rule based Symbolic Reasoning}
\author{Matthias Lochbrunner}
\date{\today}

\theoremstyle{definition}
\newtheorem{exmp}{Example}[section]

\pgfplotsset{colormap/jet}

\tikzset{
    fromColorBar/.style={
        color of colormap={1000-#1*10},
        fill=.,
    },
}

\begin{document}
\maketitle
\begin{center}
	\textbf{Abstract}
\end{center}
\begin{abstract}
	The combination of a rule based beam search and deep learning is a promising approach to solve mathematical problems.
	One obstacle to overcome is the amount of possible branches in this search as the number of applicable rules and length of the calculation increases.
	This paper proposes to address this issue by constantly shrinking the beam size with a multi-head neuronal network:
	A policy head selects rules and positions in the current term.
	A value head evaluates the probability that this calculation could lead to the desired solution.
	Both heads help to reduce the beam size such that solving advanced mathematical problems becomes feasible.
\end{abstract}

\section{Motivation}

Since current approaches to solve mathematical problems use a complex but fixed set of rules, it takes manual effort to extend their capabilities \cite{wolfram}.
The generic approach described here might be able to incorporate additional rules without requiring manual work.
Moreover, it is not limited to mathematical calculus as long as the domain can be described by a well-defined, human-readable rule set.
The correctness of the result depends only on the correctness of the given rule set.

\section{Related work}

\begin{itemize}
	\item Lampe et. al. use state of the art sequence to sequence transformers on polish notation formatted equations \cite{Lample2020Deep}.
	This approach sees solving of a mathematical problem as a translation problem on 1d data.
	They don't use the concept of rules and tree structured data in the network.
	\item Todo: Find more related work.
\end{itemize}

\section{Method}

Given a problem, e.g. solve the equation $T_1\left(x\right)\equiv T_2\left(x\right)$ for $x$,
the beam search could try to apply various rules at different nodes in the syntax tree of the equation.
The policy head proposes the most promising combination of rules and positions in the current equation.
The value head evaluates the probability of success of the current calculation step.
The cumulative result of the value head can abort the solution path of the attempt, as it "seems" unlikely that the this attempt could succeed \cite{44806}.

\subsection{Rule Application}

Every rule is of the shape $A \Longrightarrow B$ or $A \equiv B$, which is equivalent to $A \Longleftrightarrow B$ or just the combination of $A \Longrightarrow B$ and $B \Longrightarrow A$.
Without loss of correctness we just use the first shape $A \Longrightarrow B$ in this paper.
Let's name the term $T\left(x\right)$ with the depth $d$ of the corresponding syntax tree representation.
As the term can be represented by a tree we will name it's child terms $T_p$ with the position $p \in \big\{\left( n_1, n_2, \dots, n_d \right) : n_i \in \mathbb{N}, n_i \leq s \big\}$.
The root symbol of $T_p$ should be $t_p$.
Hence the sub-terms of $T$ be $T_p$ where $T_1$ for instance is the first child node of the root in that tree and $T_{1,1}$ the first child of $T_1$ and so on.
Let $s$ be the maximal number of children per node in the syntax tree.

We try to match condition $A$ of a rule at any position $p$ of term $T\left(x\right)$.
Doing so results in a list of possible positions $p_j$ with the required substitution table $M_j: S_A \to S_{T_p}$,
where $S_{T_p} \equiv \big\{ t_q : \forall q \text{ below } p\big\}$ is the set of the symbols in term $T_p$ and $S_A$ the set of symbols in the condition $A$.
Then the application of that rule results in the new terms $T^{(j)}$ where each node at $p_j$ gets replaced by the modified conclusion $B'^{(j)} \coloneqq M_j\left( B \right)$.
Where $B'^{(j)}$ is $B$ with symbols mapped back by by $M_j$. 

\begin{exmp}
To illustrate the above, let's look at the application of the derivative rule for polynomials.
% TODO: Add connection example <-> theoretcih

Consider the derivative rule
\begin{align}
	\frac{\partial}{\partial {\color{red}x}} {\color{red}x}^{\color{blue}n} \Longrightarrow {\color{blue}n}\cdot {\color{red}x}^{{\color{blue}n}-1} 
\end{align}
applied to the term
\begin{align}
	\frac{\partial}{\partial {\color{red}z}} {\color{red}z}^{\color{blue}4}
\end{align}.
Using the symbolic mapping $M\left({\color{red}x}\right) = \color{red}z$ and $M\left({\color{blue}n} \right) = {\color{blue}4}$ leads to the new term
\begin{align}
	{\color{blue}4}\cdot {\color{red}z}^{{\color{blue}4}-1} 
\end{align}.

\end{exmp}

\subsection{Network Architecture for Tree Structured Data}

Lampe et. al. showed an approach where they unroll the term $T$ using the polish notation.
Then they apply contemporary NLP translation methods on the that sequence where the resulting "translated sentence" is the next step in the calculation. \cite{Lample2020Deep}
The approach discussed here keeps the tree structure. It turned out that Tree-LSTM perform poorly on that data \cite{tai2015improved}.

% network picture

This paper will discuss the adoption of two different architectures:
Indexed convolution and attention networks with special positional encoding.

\subsubsection{Indexed Convolution}

The indexed CNN operation performs a convolution of each node on it's neighbor nodes and itself.
In order to increase the compute efficiency each term is rolled out.
Let $t_p$ be the symbol at the root of sub-term $T_p$,
then the breadth-first unrolled tern with $s=2$ is the vector $\left( t, t_1, t_1, t_{1,1},t_{1,2}, t_{2,1}, t_{2,2}, \cdots \right)$ with length $L$.
An index map $g \in \mathbb{N}^{\ell\times \left( s+2 \right)}$ carries the index of the neighbors in a constant order.


\begin{figure}[!htbp]
	\centering
	\input{docs/whitepaper/iconv.tex}
	\caption{The index tensor is an additional input of this network architecture.
		Each indexed convolution operations reads from that index tensor.
		The policy head returns for each sample a matrix of with the axis rule id and path id. 
		The output of the value head is one scalar per sample in the mini-batch. 
	}
	\label{fig:iconv_network}
\end{figure}

% Was geht rein und was geht raus
With the input channel size $i$ and output channel size $j$, the kernel weights $w \in \mathbb{R}^{\left( s+2 \right)\times i \times j}$ and bias $b \in \mathbb{R}^j$,
the indexed convolution $C$ is defined as

\begin{align}
	C\left( x, g, w, b \right)_{\ell j} \coloneqq \sum_{ki}x_{g_{\ell k}i} w_{kij}+b_j
\end{align}

where $1 \leq \ell \leq L$ is the index of symbol $t_p$ in the unrolled term. Figure \ref{fig:iconv_network}


\begin{figure}[!htbp]
	\centering
	\input{docs/whitepaper/network_output.tex}
	\caption{Policy Network output: Each row is a possible rule. Each column a sub-term $T_{p_j}$ of the initial term $T$ (highlighted in blue).
	Rules which are not applicable for the any $T_{p_j}$ are not shown here.
	This heat-map shows that dividing the equation by $a$ is more promising than blowing it up with the rules in in the in the at the bottom of the map.}
	\label{fig:network_output}
\end{figure}

Figure \ref{fig:network_output} demonstrates the output of the policy head applied on the term $\frac{b}{x}=1\cdot a$ .

\subsubsection{Transformer and Positional Encoding}

Positional encoding $PE\left( pos\right)$ in NLP is used to "attach" the position $pos$ of a word in the sentence to the embedding of that word \cite{vaswani2017attention}.
The desired properties for the positional encoding sub-terms $T_a$ and $T_b$ should be:
\begin{enumerate}[label=(\roman*)]
	\item $PE\left( T_a \right) - PE\left( T_a \right)$ is metric for the relative position of sub-terms $T_A$ and $T_B$.
	\item $PE\left( T_a \right) - PE\left( T_a \right)$ is independent of the absolute position of the sub-terms.
\end{enumerate}

A naive approach is to draw the tree on a two dimensional plane and count the sub-terms from left to right.
This approach does not fullfil any of the desired properties.

But dividing these numbers iteratively by the spread $s$.

\begin{align}
	PE^{(n+1)}\left( T \right) \coloneqq \lfloor \frac{PE^{(n)} \left( T \right)}{s} \rfloor
\end{align}

the collection of $PE^{(n+1)}\left( T \right)$ partly fulfills these properties.
This could be good enough as input for sine and cosine functions or learnable embeddings \cite{gehring2017convolutional}.


\subsection{Three-T-Loop}

The training set consists of problems generated with sympy. \cite{10.7717/peerj-cs.103}.
A training sample also has the shape of a rule $T^{(\text I)} \Longrightarrow T^{(\text T)}$ where $T^{(\text I)}$ is the starting point and $T^{(\text T)}$ the solution term.
The goal is to find the shortest chain of rule application that lead to the target term:

\begin{align}
	T^{(\text I)} \Longrightarrow T^{(j_1)} \Longrightarrow T^{(j_1j_2)} \Longrightarrow \cdots \Longrightarrow T^{(\text T)}
	\label{eq:calculation_chain}
\end{align}

To obtain trainable data from the given problem set, we use an initial beam search without the assistance of a network.
A few simple problems can already be solved and learnable data can be extracted from these solution paths.
Let's call this procedure \textit{three-T-loop} which consists of these three steps:

\begin{enumerate}[label=(\roman*)]
	\item \textbf{Try} to solve some of the training problems.
	\item \textbf{Trace} the calculation steps of the solved problems and create training data out of them.
	\item \textbf{Train} the model with the trainings data from the previous step.
\end{enumerate}

In each iteration of that \textit{three-T-loop} the number of trainable data increases. 
As the network's performance increases, it can solve more and more problems.
The trained network also reduces the amount of used rule applications as they are cancelled depending on the outcome of the value head (see Figure \ref{fig:beam_search} on page \pageref{fig:beam_search}). 


This loop can be extended by a forth step discussed in the outlook section, which allows to incorporate useful sub chains of formula (\ref{eq:calculation_chain}) into the rule set.

\begin{figure}[!htbp]
	\centering
	\input{docs/whitepaper/beam_search.tex}
	\caption{Search beam: The green trace is the correct solution.
	The red branches do not lead to a solution.
	The light nodes are skipped in later searches when the model is trained.}
	\label{fig:beam_search}
\end{figure}


\section{Results}

Todo.
% We have defined a basic rule set to perform the given problems via a scenario configuration.

\begin{figure}[!htbp]
	\centering
	Todo
	\caption{The number of solved problems is increasing as often the T3 loop is iterated.}
	\label{fig:t3loop_performance}
\end{figure}

Figure \ref{fig:t3loop_performance} on page \pageref{fig:t3loop_performance} shows the amount of solved problems increase with each iteration. 
It turned out that it is very important to suppress nonsensical rule applications, as they can blow up the beam.
To overcome this issue, adding negative training samples is very important.

% \section{Conclusion}

Although the approach described here for solving mathematical problems is hardly optimized yet,
it has shown that it is capable of solving problems from the field of mathematics.
Many extension of that approach are possible, as mentioned the "Outlook" section.


\section{Outlook}

Let us group some ideas for extending this approach into two categories:
Due to the limited time budget, many of them could not be tried out yet.
But as the basic algorithm is set up and proven to work, applications based on it's features may be feasible to accomplish in the near future.
The second group would require an extension of this core algorithm, which would take some time to implement.

\subsection{Near Future}

\subsubsection{Append Corollaries to Rule Set}

The basic rule set contains only fundamental definitions that lead to very small steps in the calculation.
These atomic steps result in long calculation chains seen in formula (\ref{eq:calculation_chain}).
However, it turned out that some parts are repeated over and over again.
These are exactly the corollaries that could be inserted into the original rule set.
To do this, you need to address three challenges:

\begin{enumerate}[label=(\roman*)]
	\item You have to evaluate which corollary is worth to store.
	\item You have to insert the new rule into the already trained network.
	\item Since there can be a large number of interesting corollaries, the training and inference time should increase less than linearly with the number of rules.
\end{enumerate}

One can overcome these challenges with a new network architecture architecture, which will be presented in a future paper.

\subsubsection{Optimizing the Network}

As mentioned earlier, the limited time budget resulted in a far from perfect network architecture.
The same is true for the time-performance of the indexed CNN operation and which would allow hyper-parameter tuning to be performed in a feasible duration.


\subsection{Far Future}

In university level mathematics many rules depend on multiple conditions.
E.g. the simple commutative property has three: $\forall a \in \mathbb{R}$ and $\forall b \in \mathbb{R}$ applies $a\cdot b \equiv a\cdot b$.
This could be represented by the concatenation via $A_1 \wedge \cdots \wedge A_n \Longrightarrow B$.
Then you are almost at propositional logic with the challenge of multiple concurrent chains of computation required for the final result.
This requires an extension to the current core algorithm. For example meta rules as \textit{reductio ad absurdum}.

Moreover, some conditions use first-order logic, which introduces the handling of complex mathematical sets.


\printbibliography
\end{document}


% Geh von Groß zu Klein
% Sehr detailiert.
% 	Was geht in die Heads rein und was kommt raus.